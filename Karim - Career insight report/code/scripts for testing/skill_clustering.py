# ==============================================================================
# SCRIPT: cluster_and_normalize.py
# PURPOSE: Loads a list of skills from a JSON file, clusters them to find
#          semantic groups, labels them using NVIDIA NIM, and analyzes the results.
# EXPECTS: An 'extracted_skills.json' file in the same directory.
# ==============================================================================

# ==============================================================================
# SECTION 0: IMPORTS
# ==============================================================================
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
import hdbscan
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from openai import OpenAI
import os
import json

# ==============================================================================
# SECTION 1: CONFIGURATION
# --- MODIFY THIS SECTION TO MATCH YOUR SETUP ---
# ==============================================================================

# -- File Paths --
# This is the JSON file generated by your 'extract_skills.py' script.
INPUT_JSON_PATH = 'extracted_skills.json'
OUTPUT_VISUALIZATION_PATH = 'skill_clusters_visualization.png'
OUTPUT_SUMMARY_CSV_PATH = 'skill_summary_ranked.csv'

# -- NVIDIA NIM CONFIGURATION --
# IMPORTANT: DO NOT HARDCODE YOUR API KEY. Use environment variables.
# In your terminal, run:
# export NVIDIA_API_KEY="nvapi-..."
# export NIM_BASE_URL="https://integrate.api.nvidia.com/v1" 
NIM_API_KEY = "OUR_NVIDIA_NIM_API_KEY_HERE"
NIM_BASE_URL = "https://integrate.api.nvidia.com/v1"
# Replace with the model ID available at your NIM endpoint
MODEL_NAME = "meta/llama-3.1-405b-instruct"


# ==============================================================================
# SECTION 2: MAIN EXECUTION LOGIC
# ==============================================================================

def main():
    """Main function to run the clustering and normalization pipeline."""
    
    print("--- Starting Skill Clustering and Normalization Script ---")

    # --- Step 1: Load Skills from JSON ---
    print(f"Loading skills from '{INPUT_JSON_PATH}'...")
    try:
        with open(INPUT_JSON_PATH, 'r') as f:
            big_list_of_skills = json.load(f)
            big_list_of_skills = list(set(big_list_of_skills))
    except FileNotFoundError:
        print(f"Error: Input file not found at '{INPUT_JSON_PATH}'.")
        print("Please run the 'extract_skills.py' script first to generate this file.")
        return
    except json.JSONDecodeError:
        print(f"Error: Could not decode JSON from '{INPUT_JSON_PATH}'. The file might be corrupted or empty.")
        return

    if not big_list_of_skills:
        print("The skills list is empty. Nothing to cluster. Exiting.")
        return
    
    print(f"Loaded {len(big_list_of_skills)} skills (including duplicates).")


    # --- Step 2: Model Initialization and Embedding Generation ---
    print("\nLoading sentence transformer model...")
    model = SentenceTransformer('all-MiniLM-L6-v2')
    
    print("Generating embeddings for all skills (this may take a moment for large lists)...")
    # We use a set to get unique skills for embedding, which is more efficient.
    unique_skills = list(set(big_list_of_skills))
    embedding_map = {skill: model.encode(skill) for skill in unique_skills}
    skill_embeddings = np.array([embedding_map[skill] for skill in unique_skills])

    # --- Step 3: Clustering ---
    print("\nClustering skill embeddings using HDBSCAN...")
    clusterer = hdbscan.HDBSCAN(
    min_cluster_size=4,
    min_samples=2,
    metric='euclidean',
    cluster_selection_epsilon=0.05,
    cluster_selection_method='leaf'  # finds fine-grained, meaningful clusters
) 
    clusterer.fit(skill_embeddings)

    df = pd.DataFrame({
        'skill': unique_skills,
        'embedding': list(skill_embeddings),
        'cluster': clusterer.labels_
    })
    
    clustered_skills_df = df[df['cluster'] != -1].copy()
    print(f"Found {clustered_skills_df['cluster'].nunique()} distinct skill clusters.")

    # --- Step 4: LLM-Based Labeling (NVIDIA NIM) ---
    print("\nLabeling clusters using NVIDIA NIM LLM...")
    if NIM_API_KEY == "YOUR_NVIDIA_API_KEY_HERE":
        raise ValueError("NVIDIA_API_KEY is not set. Please configure it in Section 1 or via environment variables.")

    client = OpenAI(base_url=NIM_BASE_URL, api_key=NIM_API_KEY)
    cluster_label_mapping = {}
    
    for cluster_id in clustered_skills_df['cluster'].unique():
        skills_in_cluster = clustered_skills_df[clustered_skills_df['cluster'] == cluster_id]['skill'].unique().tolist()
        skills_str = ", ".join(skills_in_cluster)
        
        prompt = f"""
        You are an expert technical recruiter. Based on the following list of similar skills, provide a single, concise, and professional category name.
        Skills: [{skills_str}]
        Respond with ONLY the category name and nothing else.
        Example 1: If skills are [ML, Machine Learning], respond with "Machine Learning".
        Example 2: If skills are [Docker, K8s, Kubernetes], respond with "Containerization".
        """
        
        try:
            completion = client.chat.completions.create(model=MODEL_NAME, messages=[{"role": "user", "content": prompt}], temperature=0.1, max_tokens=20)
            llm_label = completion.choices[0].message.content.strip()
            cluster_label_mapping[cluster_id] = llm_label
            print(f"  - Cluster {cluster_id} -> Labeled by LLM as '{llm_label}'")
        except Exception as e:
            print(f"  - ERROR labeling Cluster {cluster_id}: {e}")
            cluster_label_mapping[cluster_id] = f"Cluster {cluster_id} (Unlabeled)"

    clustered_skills_df['final_label'] = clustered_skills_df['cluster'].map(cluster_label_mapping)

    # --- Step 5: Analysis and Aggregation ---
    print("\nAggregating results based on LLM-generated labels...")
    
    # Map the labels back to the original full list to get accurate counts
    skill_to_label_map = pd.Series(clustered_skills_df.final_label.values, index=clustered_skills_df.skill).to_dict()
    all_skills_df = pd.DataFrame({'skill': big_list_of_skills})
    all_skills_df['final_label'] = all_skills_df['skill'].map(skill_to_label_map).fillna('Uncategorized')
    
    final_summary = all_skills_df.groupby('final_label')['skill'].agg(
        count='size',
        original_skills=lambda s: ', '.join(s.unique()) 
    ).sort_values(by='count', ascending=False).reset_index()

    # Filter out the 'Uncategorized' group from the final summary
    final_summary = final_summary[final_summary['final_label'] != 'Uncategorized']

    print("\n--- Top Skill Categories by Demand (Labeled by NVIDIA NIM) ---")
    print(final_summary.to_string())
    
    # Save summary to a CSV file for further analysis
    final_summary.to_csv(OUTPUT_SUMMARY_CSV_PATH, index=False)
    print(f"\nSaved detailed summary to '{OUTPUT_SUMMARY_CSV_PATH}'")

    # --- Step 6: Visualization ---
    if not clustered_skills_df.empty:
        print("\nCreating cluster visualization...")
        pca = PCA(n_components=2, random_state=42)
        embedding_2d = pca.fit_transform(np.stack(clustered_skills_df['embedding'].values))
        clustered_skills_df['pca_dim1'] = embedding_2d[:, 0]
        clustered_skills_df['pca_dim2'] = embedding_2d[:, 1]
        clustered_skills_df['plot_label'] = clustered_skills_df['cluster'].map(lambda cid: f"C{cid}: {cluster_label_mapping.get(cid, 'Unlabeled')}")

        plt.figure(figsize=(16, 12))
        sns.scatterplot(data=clustered_skills_df, x='pca_dim1', y='pca_dim2', hue='final_label', palette='viridis', s=100, alpha=0.9)
        plt.title('Skill Clusters Labeled via NVIDIA NIM', fontsize=16)
        plt.xlabel('PCA Component 1')
        plt.ylabel('PCA Component 2')
        plt.legend(title='Skill Category', bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.tight_layout()
        plt.savefig(OUTPUT_VISUALIZATION_PATH, dpi=300, bbox_inches='tight')
        print(f"Saved cluster visualization to '{OUTPUT_VISUALIZATION_PATH}'.")
        # plt.show() # Uncomment to display plot directly

    # --- Step 7: Final Output Lists ---
    normalized_skills_list = final_summary['final_label'].tolist()
    
    print("\n\n===================================================================")
    print("✅ Final Output 1: List of Normalized Skill Names (Ranked, Labeled by NVIDIA NIM)")
    print("===================================================================")
    print(normalized_skills_list)

    print("\n===================================================================")
    print("✅ Final Output 2: The Big List of Original Extracted Skills (Loaded from file)")
    print("===================================================================")
    print(f"(List contains {len(big_list_of_skills)} items, showing first 100)")
    print(big_list_of_skills[:100])
    
    print("\n--- Script Finished Successfully ---")

if __name__ == "__main__":
    main()